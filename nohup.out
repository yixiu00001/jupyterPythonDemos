[C 15:02:54.306 NotebookApp] No such file or directory: /data/1xiu/project/pythonPro/NotebookApp.iopub_data_rate_limit=10000000
[C 15:04:28.533 NotebookApp] No such file or directory: /data/1xiu/project/pythonPro/NotebookApp.iopub_data_rate_limit=10000000
[I 15:13:31.076 NotebookApp] Serving notebooks from local directory: /data/1xiu/project/pythonPro
[I 15:13:31.077 NotebookApp] 0 active kernels 
[I 15:13:31.077 NotebookApp] The Jupyter Notebook is running at: https://[all ip addresses on your system]:8888/
[I 15:13:31.077 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 15:13:41.471 NotebookApp] Notebook Dive_demo.ipynb is not trusted
[I 15:13:46.194 NotebookApp] Kernel started: 7a648fd9-88a6-4c8f-a78c-dd02fc830d50
[I 15:13:55.780 NotebookApp] Saving file at /Dive_demo.ipynb
[W 15:14:15.325 NotebookApp] 404 GET /api/kernels/ff6f8f8f-230c-4100-85a6-54828a49add5/channels?session_id=D6298EA1257D42B6958F06DD537DF2D6 (10.9.11.35): Kernel does not exist: ff6f8f8f-230c-4100-85a6-54828a49add5
[W 15:14:15.333 NotebookApp] 404 GET /api/kernels/ff6f8f8f-230c-4100-85a6-54828a49add5/channels?session_id=D6298EA1257D42B6958F06DD537DF2D6 (10.9.11.35) 12.05ms referer=None
[W 15:14:17.154 NotebookApp] Replacing stale connection: ff6f8f8f-230c-4100-85a6-54828a49add5:D6298EA1257D42B6958F06DD537DF2D6
[I 15:14:22.339 NotebookApp] Saving file at /Untitled4.ipynb
[I 15:14:31.346 NotebookApp] Saving file at /Dive_demo.ipynb
[I 15:15:05.633 NotebookApp] Saving file at /Untitled4.ipynb
[I 15:15:05.730 NotebookApp] Saving file at /Untitled4.ipynb
[I 15:15:06.030 NotebookApp] Saving file at /Untitled4.ipynb
[I 15:15:06.143 NotebookApp] Saving file at /Untitled4.ipynb
[I 15:15:06.412 NotebookApp] Saving file at /Untitled4.ipynb
[I 15:17:33.413 NotebookApp] Saving file at /Dive_demo.ipynb
[I 15:18:09.186 NotebookApp] Kernel started: da12ba0e-3300-4c64-937b-40efeaac8605
[I 15:18:20.337 NotebookApp] Kernel started: f0b10551-b881-47b9-a9c6-b2400a9a2e0f
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 15:19:06.257 NotebookApp] Saving file at /Dive_demo.ipynb
[I 15:19:34.951 NotebookApp] Saving file at /Dive_demo.ipynb
[I 15:19:37.917 NotebookApp] Saving file at /Dive_demo.ipynb
[I 15:19:51.472 NotebookApp] Saving file at /Dive_demo.ipynb
[I 15:20:25.482 NotebookApp] Saving file at /Untitled4.ipynb
[I 15:20:59.881 NotebookApp] Saving file at /Dive_demo.ipynb
[I 15:21:34.755 NotebookApp] Saving file at /Dive_demo.ipynb
[I 15:22:20.539 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 15:24:12.321 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 15:24:20.387 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 15:26:20.381 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 15:28:20.384 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 15:28:40 ERROR Executor: Exception in task 1.0 in stage 6.0 (TID 9)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-8-cbda38355594>", line 1, in <lambda>
TypeError: object of type 'bool' has no len()

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 15:28:40 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-8-cbda38355594>", line 1, in <lambda>
TypeError: object of type 'bool' has no len()

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 15:28:40 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 9, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<ipython-input-8-cbda38355594>", line 1, in <lambda>
TypeError: object of type 'bool' has no len()

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 15:28:40 ERROR TaskSetManager: Task 1 in stage 6.0 failed 1 times; aborting job
[I 15:29:14.818 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 15:30:20.396 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 15:32:20.388 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 15:32:56 WARN DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1064546229-10.110.13.205-1500000173116:blk_1073743480_2661
java.io.IOException: Bad response ERROR for block BP-1064546229-10.110.13.205-1500000173116:blk_1073743480_2661 from datanode DatanodeInfoWithStorage[10.110.13.206:50010,DS-aa12dab0-d3cb-49aa-9995-11329e42f8c4,DISK]
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:765)
17/07/24 15:32:56 WARN DFSClient: Error Recovery for block BP-1064546229-10.110.13.205-1500000173116:blk_1073743480_2661 in pipeline DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.206:50010,DS-aa12dab0-d3cb-49aa-9995-11329e42f8c4,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]: bad datanode DatanodeInfoWithStorage[10.110.13.206:50010,DS-aa12dab0-d3cb-49aa-9995-11329e42f8c4,DISK]
17/07/24 15:32:57 WARN DFSClient: DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:33:48 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 15:34:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 15:34:20.393 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 15:50:57.937 NotebookApp] Creating new notebook in 
[I 15:50:58.631 NotebookApp] Kernel started: ac21a1d0-7c79-4704-9fde-0e4ad0412242
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.version
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.version
[I 15:52:59.184 NotebookApp] Saving file at /Untitled4.ipynb
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/24 15:54:04 WARN AbstractLifeCycle: FAILED ServerConnector@1f294dba{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 15:54:04 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@456e3287: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 15:54:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 0) / 4]                                                                                17/07/24 15:54:19 ERROR Executor: Exception in task 2.0 in stage 1.0 (TID 6)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 123, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 15:54:19 ERROR Executor: Exception in task 3.0 in stage 1.0 (TID 7)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 123, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 15:54:19 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 5)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 123, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 15:54:19 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 4)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 123, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 15:54:19 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 5, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 123, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 15:54:19 ERROR TaskSetManager: Task 1 in stage 1.0 failed 1 times; aborting job
[I 15:54:59.174 NotebookApp] Saving file at /Untitled4.ipynb
[I 16:23:02.818 NotebookApp] Saving file at /Dive_demo.ipynb
[I 16:26:20.330 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 16:28:20.338 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:29:12 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:29:12 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:29:12 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:30:20.338 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:30:32 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:30:32 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 22)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-16-00d49a7e5529>", line 12, in <lambda>
  File "<ipython-input-16-00d49a7e5529>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:30:32 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 22, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-16-00d49a7e5529>", line 12, in <lambda>
  File "<ipython-input-16-00d49a7e5529>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:30:32 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job
17/07/24 16:30:32 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:30:32 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:30:44.003 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:30:44 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:30:44 ERROR Executor: Exception in task 0.0 in stage 17.0 (TID 23)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-17-00d49a7e5529>", line 12, in <lambda>
  File "<ipython-input-17-00d49a7e5529>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:30:44 WARN TaskSetManager: Lost task 0.0 in stage 17.0 (TID 23, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-17-00d49a7e5529>", line 12, in <lambda>
  File "<ipython-input-17-00d49a7e5529>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:30:44 ERROR TaskSetManager: Task 0 in stage 17.0 failed 1 times; aborting job
17/07/24 16:30:44 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:30:44 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:32:05.497 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:32:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:32:06 ERROR Executor: Exception in task 0.0 in stage 18.0 (TID 24)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-18-00d49a7e5529>", line 12, in <lambda>
  File "<ipython-input-18-00d49a7e5529>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:32:06 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 24, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-18-00d49a7e5529>", line 12, in <lambda>
  File "<ipython-input-18-00d49a7e5529>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:32:06 ERROR TaskSetManager: Task 0 in stage 18.0 failed 1 times; aborting job
17/07/24 16:32:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:32:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:32:20.339 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 16:33:35.687 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:33:44 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:33:45 ERROR Executor: Exception in task 0.0 in stage 19.0 (TID 25)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-19-81eaf77f44e6>", line 12, in <lambda>
  File "<ipython-input-19-81eaf77f44e6>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:33:45 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 25, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-19-81eaf77f44e6>", line 12, in <lambda>
  File "<ipython-input-19-81eaf77f44e6>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:33:45 ERROR TaskSetManager: Task 0 in stage 19.0 failed 1 times; aborting job
17/07/24 16:33:45 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:33:45 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:34:01.793 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:34:02 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:34:02 ERROR Executor: Exception in task 0.0 in stage 20.0 (TID 26)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-20-c6ab3d892762>", line 11, in <lambda>
  File "<ipython-input-20-c6ab3d892762>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:34:02 WARN TaskSetManager: Lost task 0.0 in stage 20.0 (TID 26, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-20-c6ab3d892762>", line 11, in <lambda>
  File "<ipython-input-20-c6ab3d892762>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:34:02 ERROR TaskSetManager: Task 0 in stage 20.0 failed 1 times; aborting job
17/07/24 16:34:02 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:34:02 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:34:20.346 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:34:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:34:22 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 27)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-21-a6136f644344>", line 11, in <lambda>
  File "<ipython-input-21-a6136f644344>", line 8, in func
NameError: global name 'genre_map' is not defined

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:34:22 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 27, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-21-a6136f644344>", line 11, in <lambda>
  File "<ipython-input-21-a6136f644344>", line 8, in func
NameError: global name 'genre_map' is not defined

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:34:22 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job
17/07/24 16:34:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:34:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:34:26.303 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:34:26 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:34:26 ERROR Executor: Exception in task 0.0 in stage 22.0 (TID 28)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-22-a6136f644344>", line 11, in <lambda>
  File "<ipython-input-22-a6136f644344>", line 8, in func
NameError: global name 'genre_map' is not defined

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:34:26 WARN TaskSetManager: Lost task 0.0 in stage 22.0 (TID 28, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-22-a6136f644344>", line 11, in <lambda>
  File "<ipython-input-22-a6136f644344>", line 8, in func
NameError: global name 'genre_map' is not defined

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:34:26 ERROR TaskSetManager: Task 0 in stage 22.0 failed 1 times; aborting job
17/07/24 16:34:26 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:34:26 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:34:47.803 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:34:53 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:34:53 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 29)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-23-a6136f644344>", line 11, in <lambda>
  File "<ipython-input-23-a6136f644344>", line 8, in func
NameError: global name 'genre_map' is not defined

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:34:53 WARN TaskSetManager: Lost task 0.0 in stage 23.0 (TID 29, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-23-a6136f644344>", line 11, in <lambda>
  File "<ipython-input-23-a6136f644344>", line 8, in func
NameError: global name 'genre_map' is not defined

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:34:53 ERROR TaskSetManager: Task 0 in stage 23.0 failed 1 times; aborting job
17/07/24 16:34:53 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:34:53 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:35:14 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:35:14 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:35:14 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:35:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:35:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:35:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:35:59.133 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:36:01 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:36:01 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 34)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-26-a6136f644344>", line 11, in <lambda>
  File "<ipython-input-26-a6136f644344>", line 8, in func
KeyError: '3'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:36:01 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 34, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-26-a6136f644344>", line 11, in <lambda>
  File "<ipython-input-26-a6136f644344>", line 8, in func
KeyError: '3'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:36:01 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job
17/07/24 16:36:01 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:36:01 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:36:20.338 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:36:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[u'1', u'Toy Story (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', u'0', u'0', u'0', u'1', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0']
[3, 4, 5]
17/07/24 16:36:34 ERROR Executor: Exception in task 0.0 in stage 27.0 (TID 35)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-27-ba2f318fec10>", line 13, in <lambda>
  File "<ipython-input-27-ba2f318fec10>", line 10, in func
KeyError: '3'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:36:34 WARN TaskSetManager: Lost task 0.0 in stage 27.0 (TID 35, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-27-ba2f318fec10>", line 13, in <lambda>
  File "<ipython-input-27-ba2f318fec10>", line 10, in func
KeyError: '3'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:36:34 ERROR TaskSetManager: Task 0 in stage 27.0 failed 1 times; aborting job
17/07/24 16:36:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:36:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:36:46.590 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 16:36:47 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[u'1', u'Toy Story (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', u'0', u'0', u'0', u'1', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0']
[3, 4, 5]
17/07/24 16:36:47 ERROR Executor: Exception in task 0.0 in stage 28.0 (TID 36)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-28-ba2f318fec10>", line 13, in <lambda>
  File "<ipython-input-28-ba2f318fec10>", line 10, in func
KeyError: '3'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 16:36:47 WARN TaskSetManager: Lost task 0.0 in stage 28.0 (TID 36, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-28-ba2f318fec10>", line 13, in <lambda>
  File "<ipython-input-28-ba2f318fec10>", line 10, in func
KeyError: '3'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 16:36:47 ERROR TaskSetManager: Task 0 in stage 28.0 failed 1 times; aborting job
17/07/24 16:36:47 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 16:36:47 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 16:38:20.338 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 16:39:03.087 NotebookApp] Saving file at /Dive_demo.ipynb
17/07/24 17:03:40 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:03:40 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:03:40 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:03:45 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:03:45 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:03:45 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:03:52 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:03:52 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:03:52 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:04:03.701 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:04:20.315 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:04:30.779 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:04:31 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:04:31 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:04:31 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:04:58.231 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:04:58 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:04:58 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:04:58 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:06:20.327 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:06:28.683 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:06:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:34 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:35 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:35 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:35 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:06:54.365 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:06:54 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[u'1', u'Toy Story (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', u'0', u'0', u'0', u'1', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0']
[3, 4, 5]
[u'2', u'GoldenEye (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?GoldenEye%20(1995)', u'0', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0']
[1, 2, 16]
[u'3', u'Four Rooms (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0']
[16]
[u'4', u'Get Shorty (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)', u'0', u'1', u'0', u'0', u'0', u'1', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0']
[1, 5, 8]
[u'5', u'Copycat (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Copycat%20(1995)', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0']
[6, 8, 16]
17/07/24 17:06:54 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:06:54 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:03 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:04 ERROR Executor: Exception in task 0.0 in stage 39.0 (TID 49)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-40-c6ab3d892762>", line 11, in <lambda>
  File "<ipython-input-40-c6ab3d892762>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:07:04 WARN TaskSetManager: Lost task 0.0 in stage 39.0 (TID 49, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-40-c6ab3d892762>", line 11, in <lambda>
  File "<ipython-input-40-c6ab3d892762>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 17:07:04 ERROR TaskSetManager: Task 0 in stage 39.0 failed 1 times; aborting job
17/07/24 17:07:04 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:04 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:07:17.313 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:07:17 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:17 ERROR Executor: Exception in task 0.0 in stage 40.0 (TID 50)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-41-c6ab3d892762>", line 11, in <lambda>
  File "<ipython-input-41-c6ab3d892762>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:07:17 WARN TaskSetManager: Lost task 0.0 in stage 40.0 (TID 50, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-41-c6ab3d892762>", line 11, in <lambda>
  File "<ipython-input-41-c6ab3d892762>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 17:07:17 ERROR TaskSetManager: Task 0 in stage 40.0 failed 1 times; aborting job
17/07/24 17:07:17 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:17 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:07:45.572 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:07:46 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:46 ERROR Executor: Exception in task 0.0 in stage 41.0 (TID 51)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-42-4a5af4bd29a8>", line 11, in <lambda>
  File "<ipython-input-42-4a5af4bd29a8>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:07:46 WARN TaskSetManager: Lost task 0.0 in stage 41.0 (TID 51, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-42-4a5af4bd29a8>", line 11, in <lambda>
  File "<ipython-input-42-4a5af4bd29a8>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 17:07:46 ERROR TaskSetManager: Task 0 in stage 41.0 failed 1 times; aborting job
17/07/24 17:07:46 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:46 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:57 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:57 ERROR Executor: Exception in task 0.0 in stage 42.0 (TID 52)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-43-177aa43b094f>", line 11, in <lambda>
  File "<ipython-input-43-177aa43b094f>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:07:57 WARN TaskSetManager: Lost task 0.0 in stage 42.0 (TID 52, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-43-177aa43b094f>", line 11, in <lambda>
  File "<ipython-input-43-177aa43b094f>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 17:07:57 ERROR TaskSetManager: Task 0 in stage 42.0 failed 1 times; aborting job
17/07/24 17:07:57 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:07:57 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:08:11.124 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:08:11 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:08:11 ERROR Executor: Exception in task 1.0 in stage 43.0 (TID 54)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 346, in func
    return f(iterator)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1041, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1041, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-44-689afae891d3>", line 11, in <lambda>
  File "<ipython-input-44-689afae891d3>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:08:11 WARN TaskSetManager: Lost task 1.0 in stage 43.0 (TID 54, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 346, in func
    return f(iterator)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1041, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1041, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-44-689afae891d3>", line 11, in <lambda>
  File "<ipython-input-44-689afae891d3>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 17:08:11 ERROR Executor: Exception in task 0.0 in stage 43.0 (TID 53)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 2423, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 346, in func
    return f(iterator)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1041, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1041, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "<ipython-input-44-689afae891d3>", line 11, in <lambda>
  File "<ipython-input-44-689afae891d3>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:08:11 ERROR TaskSetManager: Task 1 in stage 43.0 failed 1 times; aborting job
17/07/24 17:08:11 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:08:11 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:08:16.464 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:18:13.438 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:18:13 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:18:13 ERROR Executor: Exception in task 0.0 in stage 44.0 (TID 55)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-45-e404f55c487b>", line 11, in <lambda>
  File "<ipython-input-45-e404f55c487b>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:18:13 WARN TaskSetManager: Lost task 0.0 in stage 44.0 (TID 55, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-45-e404f55c487b>", line 11, in <lambda>
  File "<ipython-input-45-e404f55c487b>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 17:18:13 ERROR TaskSetManager: Task 0 in stage 44.0 failed 1 times; aborting job
17/07/24 17:18:13 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:18:13 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:18:20.320 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:19:35.589 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:19:36 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:19:36 ERROR Executor: Exception in task 0.0 in stage 45.0 (TID 56)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-47-201d9a222a19>", line 13, in <lambda>
  File "<ipython-input-47-201d9a222a19>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:19:36 WARN TaskSetManager: Lost task 0.0 in stage 45.0 (TID 56, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-47-201d9a222a19>", line 13, in <lambda>
  File "<ipython-input-47-201d9a222a19>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 17:19:36 ERROR TaskSetManager: Task 0 in stage 45.0 failed 1 times; aborting job
17/07/24 17:19:36 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:19:36 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:19:52 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:19:52 ERROR Executor: Exception in task 0.0 in stage 46.0 (TID 57)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-48-52a0f2e49998>", line 11, in <lambda>
  File "<ipython-input-48-52a0f2e49998>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/24 17:19:52 WARN TaskSetManager: Lost task 0.0 in stage 46.0 (TID 57, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 172, in main
    process()
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 167, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/rdd.py", line 1339, in takeUpToNumLeft
    yield next(iterator)
  File "<ipython-input-48-52a0f2e49998>", line 11, in <lambda>
  File "<ipython-input-48-52a0f2e49998>", line 2, in getName
AttributeError: 'list' object has no attribute 'map'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/24 17:19:52 ERROR TaskSetManager: Task 0 in stage 46.0 failed 1 times; aborting job
17/07/24 17:19:52 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:19:52 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:20:13 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[u'1', u'Toy Story (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', u'0', u'0', u'0', u'1', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0']
[3, 4, 5]
[u'2', u'GoldenEye (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?GoldenEye%20(1995)', u'0', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0']
[1, 2, 16]
[u'3', u'Four Rooms (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0']
[16]
[u'4', u'Get Shorty (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)', u'0', u'1', u'0', u'0', u'0', u'1', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0']
[1, 5, 8]
[u'5', u'Copycat (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Copycat%20(1995)', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0']
[6, 8, 16]
17/07/24 17:20:13 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:20:13 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:20:20.321 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:20:26.933 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:20:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
index= [3, 4, 5]
index= [1, 2, 16]
index= [16]
index= [1, 5, 8]
index= [6, 8, 16]
17/07/24 17:20:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:20:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:21:09.844 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:21:15 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:21:15 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:21:15 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:21:15 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:21:15 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:21:15 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:21:27.836 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:21:28 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:21:28 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:21:28 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:22:20.316 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:31:47.480 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:32:19 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:19 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:19 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:19 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:32:20.312 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:32:20 WARN Executor: 1 block locks were not released by TID = 64:
[rdd_73_0]
17/07/24 17:32:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 55:=============================>                            (1 + 1) / 2]17/07/24 17:32:21 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:21 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
                                                                                17/07/24 17:32:21 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:21 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:21 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:21 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:22 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
17/07/24 17:32:22 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
17/07/24 17:32:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:22 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
17/07/24 17:32:22 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
17/07/24 17:32:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:22 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 72:>                                                         (0 + 0) / 4]17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
                                                                                17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:23 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 WARN Executor: 1 block locks were not released by TID = 132:
[rdd_192_0]
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 WARN Executor: 1 block locks were not released by TID = 133:
[rdd_193_0]
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 WARN Executor: 1 block locks were not released by TID = 134:
[rdd_192_0]
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 WARN Executor: 1 block locks were not released by TID = 135:
[rdd_193_0]
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:32:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:34:20.314 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:34:26.275 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:06 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:07 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:09 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:44:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:44:20.315 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[I 17:47:26.492 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:47:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:48:20.308 NotebookApp] Saving file at /ClusterTestSpark.ipynb
17/07/24 17:48:26 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:48:26 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:48:26 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:48:26 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:48:28 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:48:29 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:48:29 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:49:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:49:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:49:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:49:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:49:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/07/24 17:49:27 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]], original=[DatanodeInfoWithStorage[10.110.13.207:50010,DS-39496589-ab21-41d3-aced-e45849c3c648,DISK], DatanodeInfoWithStorage[10.110.13.205:50010,DS-7f19a741-e111-443b-9fb8-bc9f0ba04308,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[I 17:50:20.308 NotebookApp] Saving file at /ClusterTestSpark.ipynb
[W 07:56:55.559 NotebookApp] Replacing stale connection: 7a648fd9-88a6-4c8f-a78c-dd02fc830d50:5D81EB9162D843C98897B34F92CDE2E5
[W 08:09:21.020 NotebookApp] WebSocket ping timeout after 119997 ms.
[W 08:09:23.137 NotebookApp] WebSocket ping timeout after 119993 ms.
[W 08:21:39.841 NotebookApp] WebSocket ping timeout after 119929 ms.
[W 08:23:37.310 NotebookApp] WebSocket ping timeout after 115242 ms.
[W 08:26:18.271 NotebookApp] WebSocket ping timeout after 112593 ms.
[I 14:20:09.648 NotebookApp] 302 GET / (10.9.11.35) 0.69ms
[W 17:52:03.377 NotebookApp] Session not found: session_id=u'6fc5589e-4406-4e51-80ed-cfad9387acff'
[W 17:52:03.378 NotebookApp] 404 DELETE /api/sessions/6fc5589e-4406-4e51-80ed-cfad9387acff (10.9.11.123) 28.32ms referer=https://10.110.13.207:8888/notebooks/recommendSpark.ipynb
[I 17:52:03.408 NotebookApp] Kernel started: 86887f0c-a6bb-4a72-8322-a17ff737e38b
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.version
[I 17:52:12.120 NotebookApp] Kernel restarted: 86887f0c-a6bb-4a72-8322-a17ff737e38b
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.version
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.version
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/25 17:52:23 WARN AbstractLifeCycle: FAILED ServerConnector@740c9f39{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/25 17:52:23 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@7930e675: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/25 17:52:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/07/25 17:52:23 WARN AbstractLifeCycle: FAILED ServerConnector@6154497a{HTTP/1.1}{0.0.0.0:4041}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/25 17:52:23 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@2340603d: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/25 17:52:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[I 17:54:17.626 NotebookApp] Saving file at /recommendSpark.ipynb
[I 18:10:17.616 NotebookApp] Saving file at /recommendSpark.ipynb
[W 18:23:47.479 NotebookApp] WebSocket ping timeout after 119997 ms.
[W 07:50:49.158 NotebookApp] Replacing stale connection: 86887f0c-a6bb-4a72-8322-a17ff737e38b:F055CE0B7DFC454BA6E12CCB7FB67C21
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.version
[I 08:46:01.627 NotebookApp] 302 GET / (10.72.179.113) 0.97ms
[I 08:46:35.715 NotebookApp] Kernel started: 1135a473-c508-4d9c-89e1-3807699a95ad
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/26 08:46:56 WARN AbstractLifeCycle: FAILED ServerConnector@19b8fd4b{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 08:46:56 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@1e9fbe13: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 08:46:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/07/26 08:46:56 WARN AbstractLifeCycle: FAILED ServerConnector@1570ba4e{HTTP/1.1}{0.0.0.0:4041}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 08:46:56 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@39f18f07: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 08:46:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/07/26 08:46:56 WARN AbstractLifeCycle: FAILED ServerConnector@785aaf78{HTTP/1.1}{0.0.0.0:4042}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 08:46:56 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@629ac920: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 08:46:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.version
[I 08:59:03.606 NotebookApp] Saving file at /recommendSpark.ipynb
[I 09:05:00.307 NotebookApp] Creating new notebook in 
[I 09:05:08.075 NotebookApp] Kernel started: c08c0657-bd35-4136-b4ec-a433ae0a2875
[W 09:06:04.602 NotebookApp] Notebook demoSparkPreprocess.ipynb is not trusted
[I 09:06:06.069 NotebookApp] Kernel started: ac5ebdfd-133d-4a57-9dae-b479ffe2be9a
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/26 09:06:22 WARN AbstractLifeCycle: FAILED ServerConnector@1f294dba{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:06:22 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@456e3287: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:06:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/07/26 09:06:22 WARN AbstractLifeCycle: FAILED ServerConnector@39f18f07{HTTP/1.1}{0.0.0.0:4041}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:06:22 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@66c5884d: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:06:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/07/26 09:06:22 WARN AbstractLifeCycle: FAILED ServerConnector@629ac920{HTTP/1.1}{0.0.0.0:4042}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:06:22 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@79552ec7: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:06:22 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/07/26 09:06:22 WARN AbstractLifeCycle: FAILED ServerConnector@5fd3fbd8{HTTP/1.1}{0.0.0.0:4043}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:06:22 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@6340bb9b: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:06:22 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[I 09:07:00.141 NotebookApp] Kernel shutdown: c08c0657-bd35-4136-b4ec-a433ae0a2875
[I 09:07:00.221 NotebookApp] Kernel started: 712500dc-42fa-4b1f-ae67-9665a574ea8d
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.version
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/26 09:07:07 WARN AbstractLifeCycle: FAILED ServerConnector@6b3d1257{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:07 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@145ad4ff: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/07/26 09:07:07 WARN AbstractLifeCycle: FAILED ServerConnector@461ba756{HTTP/1.1}{0.0.0.0:4041}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:07 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@63c0f4f6: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/07/26 09:07:07 WARN AbstractLifeCycle: FAILED ServerConnector@20546cd8{HTTP/1.1}{0.0.0.0:4042}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:07 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@4e2dc8e7: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/07/26 09:07:07 WARN AbstractLifeCycle: FAILED ServerConnector@527aa9dc{HTTP/1.1}{0.0.0.0:4043}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:07 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@4a7512dc: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:07 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[I 09:07:08.108 NotebookApp] Saving file at /Untitled5.ipynb
17/07/26 09:07:11 WARN BlockReaderFactory: BlockReaderFactory(fileName=/data/ml-100k/u.user, block=BP-1064546229-10.110.13.205-1500000173116:blk_1073742765_1942): I/O error requesting file descriptors.  Disabling domain socket DomainSocket(fd=308,path=/var/lib/hadoop-hdfs/dn_socket)
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.FileChannelImpl.size(FileChannelImpl.java:315)
	at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm.getUsableLength(ShortCircuitShm.java:75)
	at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm.<init>(ShortCircuitShm.java:481)
	at org.apache.hadoop.hdfs.shortcircuit.DfsClientShm.<init>(DfsClientShm.java:70)
	at org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.requestNewShm(DfsClientShmManager.java:188)
	at org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.allocSlot(DfsClientShmManager.java:262)
	at org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.allocSlot(DfsClientShmManager.java:434)
	at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.allocShmSlot(ShortCircuitCache.java:1015)
	at org.apache.hadoop.hdfs.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:478)
	at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.create(ShortCircuitCache.java:782)
	at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.fetchOrCreate(ShortCircuitCache.java:716)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getBlockReaderLocal(BlockReaderFactory.java:422)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:333)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:208)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:246)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:211)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/07/26 09:07:11 WARN ShortCircuitCache: ShortCircuitCache(0x6bb99d04): failed to load 1073742765_BP-1064546229-10.110.13.205-1500000173116
17/07/26 09:07:11 WARN BlockReaderFactory: I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:208)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:246)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:211)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/07/26 09:07:11 WARN DFSClient: Failed to connect to /10.110.13.207:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:208)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:246)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:211)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/07/26 09:07:11 WARN BlockReaderFactory: I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:208)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:246)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:211)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/07/26 09:07:11 WARN DFSClient: Failed to connect to /10.110.13.205:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:208)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:246)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:211)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/07/26 09:07:11 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2876.800089413811 msec.
17/07/26 09:07:11 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 123, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:07:11 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/hdp/2.5.0.0-1245/spark2/python/lib/pyspark.zip/pyspark/worker.py", line 123, in main
    ("%d.%d" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/07/26 09:07:11 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[I 09:09:08.141 NotebookApp] Saving file at /Untitled5.ipynb
[I 09:32:24.269 NotebookApp] Kernel shutdown: 712500dc-42fa-4b1f-ae67-9665a574ea8d
[I 09:32:24.364 NotebookApp] Kernel started: 66990e3b-0f2c-496a-a95f-3fca06d4cc11
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/26 09:32:33 WARN AbstractLifeCycle: FAILED ServerConnector@19b8fd4b{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:32:33 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@1e9fbe13: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:32:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/07/26 09:32:33 WARN AbstractLifeCycle: FAILED ServerConnector@1570ba4e{HTTP/1.1}{0.0.0.0:4041}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:32:33 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@39f18f07: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:32:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/07/26 09:32:33 WARN AbstractLifeCycle: FAILED ServerConnector@785aaf78{HTTP/1.1}{0.0.0.0:4042}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:32:33 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@629ac920: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:32:33 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/07/26 09:32:33 WARN AbstractLifeCycle: FAILED ServerConnector@6221a0e2{HTTP/1.1}{0.0.0.0:4043}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:32:33 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@5fd3fbd8: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:349)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:359)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2195)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2186)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:359)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:140)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:460)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:236)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
17/07/26 09:32:33 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[I 09:33:08.162 NotebookApp] Saving file at /Untitled5.ipynb
