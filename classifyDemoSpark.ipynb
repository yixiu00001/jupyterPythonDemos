{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-ffd50910b931>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-ffd50910b931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-ffd50910b931>:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import  SparkContext,SparkConf\n",
    "sc = SparkContext( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/hdp/2.5.0.0-1245/spark2/python/py4j/java_gateway.py\", line 883, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/hdp/2.5.0.0-1245/spark2/python/py4j/java_gateway.py\", line 1040, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o20.sc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-9ce4a44dd320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrawData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/kaggle/train_noheader.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrawData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mtextFile\u001b[0;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34mu'Hello world!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         return RDD(self._jsc.textFile(name, minPartitions), self,\n\u001b[1;32m    533\u001b[0m                    UTF8Deserializer(use_unicode))\n",
      "\u001b[0;32m/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         reduce tasks)\n\u001b[1;32m    390\u001b[0m         \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.0.0-1245/spark2/python/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.0.0-1245/spark2/python/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise Py4JError(\n\u001b[1;32m    326\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o20.sc"
     ]
    }
   ],
   "source": [
    "rawData = sc.textFile('/data/kaggle/train_noheader.tsv')\n",
    "records = rawData.map(lambda x: x.split('\\t'))\n",
    "print records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]), LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]), LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]), LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369])]\n",
      "[u'0', u'1', u'1', u'1']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "#过滤“符号，替换为空\n",
    "trimmed = records.map(lambda item :[xx.replace('\\\"', '' ) for xx in item])\n",
    "#获取最后一位\n",
    "label = trimmed.map(lambda item: item[-1])\n",
    "label = trimmed.map(lambda x : x[-1])\n",
    "#数据处理，最后一位为key，从第四列开始-最后为value\n",
    "#如果数值为？则置为0，如果为“ 置为空\n",
    "#将x类型转换为整形，y的每一个值转为float类型\n",
    "#将x和y构造成LabeledPoint结构\n",
    "data = trimmed.map(lambda x : (x[-1],x[4:-1])).\\\n",
    "               map(lambda (x,y):(x.replace('\\\"',''),[0.0 if yy=='\\\"?\\\"' else yy.replace('\\\"','') for yy in y])).\\\n",
    "               map(lambda (x,y):(x.replace(\"\\\"\",\"\"),[0.0 if yy =='?' else yy.replace(\"\\\"\",\"\") for yy in y])).\\\n",
    "               map(lambda (x,y):(int(x), [float(yy) for yy in y])).\\\n",
    "               map(lambda (x,y):LabeledPoint(x,Vectors.dense(y)))\n",
    "print data.take(4)\n",
    "print label.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7395"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]),\n",
       " LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]),\n",
       " LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndata = trimmed.map(lambda x : (x[-1],x[4:-1])).\\\n",
    "               map(lambda (x,y):(x.replace('\\\"',''),[0.0 if yy=='\\\"?\\\"' else yy.replace('\\\"','') for yy in y])).\\\n",
    "               map(lambda (x,y):(x.replace(\"\\\"\",\"\"),[0.0 if yy =='?' else yy.replace(\"\\\"\",\"\") for yy in y])).\\\n",
    "               map(lambda (x,y):(int(x), [float(yy)  for yy in y])).\\\n",
    "               map(lambda (x,y):(int(x), [0 if yy<0 else yy for yy in y])).\\\n",
    "            map(lambda (x,y):LabeledPoint(x,Vectors.dense(y)))\n",
    "ndata.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class nums: 2\n",
      "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])]\n",
      "LogisticRegressionWithSGD= (weights=[-0.550608763357,-2.28562752154,-0.439331166963,-0.162228223433,-0.0292143091078,-0.0126114724381,-5.17235106843,0.207942323454,0.0,-0.129097852708,-0.0311426509567,-0.301522439418,-0.619933705283,-0.726666707707,-0.704681632452,-62.6756345609,-0.076132956751,-179.237173723,-99.5223009574,-6.87205576007,-0.191685436051,-0.159520259813], intercept=0.0)\n",
      "SVMWithSGD= (weights=[-0.574551503125,-2.39015988071,-0.457233396112,-0.168485986067,-0.0298275974716,-0.0127395129754,-5.40743227025,0.217112420397,0.0,-0.13512838633,-0.0326565313643,-0.314805369155,-0.651196600765,-0.757096629994,-0.734668872816,-65.6071533534,-0.0794581311037,-163.709851544,-103.599245578,-7.17990782672,-0.20025471291,-0.166705251117], intercept=0.0)\n",
      "NativeBayes= <pyspark.mllib.classification.NaiveBayesModel object at 0x7fd5501a1190>\n",
      "DecisionTree= DecisionTreeModel classifier of depth 10 with 731 nodes\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "#导入相应的类\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "numIteration = 100\n",
    "maxTreeDepth = 10\n",
    "\n",
    "numClass = label.distinct().count() \n",
    "print \"The class nums:\", numClass\n",
    "#训练逻辑回归、支持向量机、朴素贝叶斯和决策树模型\n",
    "lrModel = LogisticRegressionWithSGD.train(data, numIteration)\n",
    "svmModel = SVMWithSGD.train(data, numIteration)\n",
    "nbModel = NaiveBayes.train(ndata)\n",
    "print data.take(1)\n",
    "dtModel = DecisionTree.trainClassifier(data, numClass, {}, impurity='entropy',maxDepth=maxTreeDepth)\n",
    "print \"LogisticRegressionWithSGD=\", lrModel\n",
    "print \"SVMWithSGD=\", svmModel\n",
    "print \"NativeBayes=\", nbModel\n",
    "print \"DecisionTree=\", dtModel\n",
    "#逻辑回归的模型和svm的模型权重好像啊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])\n"
     ]
    }
   ],
   "source": [
    "#预测\n",
    "testData = ndata.first()\n",
    "print testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n"
     ]
    }
   ],
   "source": [
    "print lrModel.predict(testData.features), testData.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0, 0, 0, 0, 0] [0.0, 1.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "predicts = svmModel.predict(data.map(lambda item:item.features))\n",
    "print predicts.take(5), data.map(lambda label:label.label).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all= 7395\n",
      "lr= 0.486680189317\n",
      "svm= 0.486680189317\n",
      "nb= 0.580392156863\n",
      "dt= 0.764435429344\n"
     ]
    }
   ],
   "source": [
    "allCount = data.count()\n",
    "lrCorrectCount = data.map(lambda item :1 if lrModel.predict(item.features)==item.label else 0).sum()\n",
    "svmCorrectCount = data.map(lambda item: 1 if svmModel.predict(item.features) == item.label else 0).sum()\n",
    "nbCorrectCount = ndata.map(lambda item: 1 if nbModel.predict(item.features)==item.label else 0).sum()\n",
    "#决策树模型的预测正确数目\n",
    "predictLabel= dtModel.predict(data.map(lambda point: point.features)).collect()\n",
    "trueLabel = data.map(lambda point: point.label).collect()\n",
    "dtCorrectCount = sum([1.0 if prediction == trueLabel[i] else 0.0 for i, prediction in enumerate(predictLabel)])\n",
    "print \"all=\",allCount\n",
    "print \"lr=\",lrCorrectCount/(allCount*1.0)\n",
    "print \"svm=\", svmCorrectCount*1.0/allCount\n",
    "print \"nb=\", nbCorrectCount*1.0/allCount\n",
    "print \"dt=\", dtCorrectCount*1.0/allCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
