{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-ffd50910b931>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ffd50910b931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/hdp/2.5.0.0-1245/spark2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-ffd50910b931>:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import  SparkContext,SparkConf\n",
    "sc = SparkContext( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[u'196', u'242', u'3', u'881250949'],\n",
       " [u'186', u'302', u'3', u'891717742'],\n",
       " [u'22', u'377', u'1', u'878887116'],\n",
       " [u'244', u'51', u'2', u'880606923'],\n",
       " [u'166', u'346', u'1', u'886397596']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读文件，存储在hdfs的/data目录下\n",
    "user_data = sc.textFile(\"/data/ml-100k/u.data\")\n",
    "#取第一行,用户id，影片id， 评分，时间戳\n",
    "print user_data.first()\n",
    "rawRatings = user_data.map(lambda x: x.split('\\t'))\n",
    "rawRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rating(user=196, product=242, rating=3.0), Rating(user=186, product=302, rating=3.0), Rating(user=22, product=377, rating=1.0), Rating(user=244, product=51, rating=2.0), Rating(user=166, product=346, rating=1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating, ALS\n",
    "#\n",
    "ratings = rawRatings.map(lambda x: Rating(int(x[0]),int(x[1]),float(x[2])))\n",
    "print ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 943 1682\n"
     ]
    }
   ],
   "source": [
    "#rank、iterations和lambda参数的值分别为50、10和0.01\n",
    "model = ALS.train(ratings, 50, 10, 0.01)\n",
    "userFeatures = model.userFeatures()\n",
    "produceFeatures = model.productFeatures()\n",
    "print userFeatures.count(), produceFeatures.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1038707423\n",
      "Rating(user=789, product=188, rating=5.940408255489615)\n",
      "Rating(user=789, product=179, rating=5.751181351967185)\n",
      "Rating(user=789, product=53, rating=5.486470919042393)\n",
      "Rating(user=789, product=183, rating=5.412903788393566)\n",
      "Rating(user=789, product=346, rating=5.392252949410973)\n",
      "Rating(user=789, product=428, rating=5.384187983039647)\n",
      "Rating(user=789, product=56, rating=5.360567481659956)\n",
      "Rating(user=789, product=144, rating=5.301477900696691)\n",
      "Rating(user=789, product=855, rating=5.239291510043716)\n",
      "Rating(user=789, product=523, rating=5.222502891023609)\n"
     ]
    }
   ],
   "source": [
    "#预测用户对电影的评分\n",
    "predictRating = model.predict(789,123)\n",
    "print predictRating\n",
    "#获取用户评分最高的n个物品\n",
    "topKRecs = model.recommendProducts(789,10)\n",
    "for rec in topKRecs:\n",
    "    print rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 5.0\n",
      "475 5.0\n",
      "9 5.0\n",
      "50 5.0\n",
      "150 5.0\n",
      "276 5.0\n",
      "129 5.0\n",
      "100 5.0\n",
      "741 5.0\n",
      "1012 4.0\n",
      "93 4.0\n",
      "293 4.0\n",
      "181 4.0\n",
      "1008 4.0\n",
      "508 4.0\n",
      "1007 4.0\n",
      "124 4.0\n",
      "1161 3.0\n",
      "294 3.0\n",
      "1 3.0\n",
      "284 3.0\n",
      "1017 3.0\n",
      "111 3.0\n",
      "742 3.0\n",
      "248 3.0\n",
      "249 3.0\n",
      "591 3.0\n",
      "288 3.0\n",
      "762 3.0\n",
      "628 3.0\n",
      "137 2.0\n",
      "151 2.0\n",
      "286 1.0\n"
     ]
    }
   ],
   "source": [
    "#查看某一用户评级过的电影\n",
    "moviesForUser = ratings.groupBy(lambda x : x.user).mapValues(list).lookup(789)\n",
    "for i in sorted(moviesForUser[0],key=lambda x : x.rating,reverse=True):\n",
    "    print i.product, i.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank:0 Full Metal Jacket (1987):5.94040825549\n",
      "rank:1 Clockwork Orange, A (1971):5.75118135197\n",
      "rank:2 Natural Born Killers (1994):5.48647091904\n",
      "rank:3 Alien (1979):5.41290378839\n",
      "rank:4 Jackie Brown (1997):5.39225294941\n",
      "rank:5 Harold and Maude (1971):5.38418798304\n",
      "rank:6 Pulp Fiction (1994):5.36056748166\n",
      "rank:7 Die Hard (1988):5.3014779007\n",
      "rank:8 Diva (1981):5.23929151004\n",
      "rank:9 Cool Hand Luke (1967):5.22250289102\n"
     ]
    }
   ],
   "source": [
    "#load movie info\n",
    "movies = sc.textFile(\"/data/ml-100k/u.item\")\n",
    "titles = movies.map(lambda line: (int(line.split('|')[0]),line.split('|')[1])).collectAsMap()\n",
    "#print titles.values()\n",
    "#print movies.take(5)\n",
    "for i,rec in enumerate(topKRecs):\n",
    "    print 'rank:'+str(i)+' '+str(titles[rec.product])+':'+str(rec.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 0.63083138014281248), (8, 0.62569870987802167)]\n",
      "[(123, 1.0), (204, 0.80989080786831025), (172, 0.79816406253966676), (176, 0.7972386027847993), (210, 0.79215369116689671)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#cosin similary\n",
    "def cosSimilarity(x,y):\n",
    "    #norm ||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}\n",
    "     return np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "itemId = 123\n",
    "itemFactor = model.productFeatures().lookup(itemId)[0]\n",
    "#print itemFactor\n",
    "sims = model.productFeatures().map(lambda (movieID, factor):(movieID, cosSimilarity(np.array(factor), np.array(itemFactor))))\n",
    "print sims.take(2)\n",
    "\n",
    "#desent sort\n",
    "#sims.sortBy(lambda (x,y):y,ascending=False).take(10)\n",
    "simsSorted = sims.sortBy(lambda (x,y):y,ascending=False)\n",
    "print simsSorted.take(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(user=789, product=1012, rating=4.0)\n",
      "real= 4.0 preRating= 3.9789836777 err= 0.000441685803038\n"
     ]
    }
   ],
   "source": [
    "print moviesForUser[0][0]\n",
    "actual = moviesForUser[0][0];\n",
    "realRating = actual.rating;\n",
    "preRating = model.predict(789, actual.product)\n",
    "squaredError = np.power(realRating-preRating,2)\n",
    "print \"real=\",realRating,\"preRating=\", preRating,\"err=\",squaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real score: [(196, 242), (186, 302), (22, 377), (244, 51), (166, 346)]\n",
      "Rating(user=316, product=1084, rating=3.9773565963470854)\n",
      "predict score : [((316, 1084), 3.9773565963470854), ((504, 1084), 4.010033325125923), ((424, 1084), 4.976733131183338), ((541, 1084), 4.010127099719102), ((181, 1084), 2.0356360821308934)]\n",
      "组合预测的评分和实际的评分: [((711, 707), (5.0, 4.921906083347011)), ((650, 622), (3.0, 3.131526366462754)), ((472, 584), (1.0, 1.5695049990572358)), ((752, 316), (3.0, 3.094518647981753)), ((18, 428), (3.0, 3.4571667279255007))]\n",
      "模型的均方误差: 0.0845549292727\n",
      "模型的均方根误差: 0.290783302947\n"
     ]
    }
   ],
   "source": [
    "userProducts = ratings.map(lambda rating:(rating.user, rating.product))\n",
    "print 'real score:',userProducts.take(5)\n",
    "\n",
    "#预测所有用户对电影的相应评分\n",
    "print model.predictAll(userProducts).collect()[0]\n",
    "predictions = model.predictAll(userProducts).map(lambda rating:((rating.user,rating.product), rating.rating))\n",
    "print 'predict score :',predictions.take(5)\n",
    "\n",
    "ratingsAndPredictions = ratings.map(lambda rating:((rating.user,rating.product),rating.rating)).join(predictions)\n",
    "print '组合预测的评分和实际的评分:',ratingsAndPredictions.take(5)\n",
    "\n",
    "MSE = ratingsAndPredictions.map(lambda ((x,y),(m,n)):np.power(m-n,2)).reduce(lambda x,y:x+y)/ratingsAndPredictions.count() \n",
    "print '模型的均方误差:',MSE \n",
    "print '模型的均方根误差:',np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5.0, 4.921906083347011), (3.0, 3.131526366462754), (1.0, 1.5695049990572358), (3.0, 3.094518647981753), (3.0, 3.4571667279255007)]\n",
      "均方误差 = 0.084555\n",
      "均方根误差 = 0.290783\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "predictedAndTrue = ratingsAndPredictions.map(lambda ((userId,product),(predicted, actual))\n",
    "      :(predicted,actual))\n",
    "print predictedAndTrue.take(5)\n",
    "regressionMetrics = RegressionMetrics(predictedAndTrue)\n",
    "print \"均方误差 = %f\"%regressionMetrics.meanSquaredError\n",
    "print \"均方根误差 = %f\"% regressionMetrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
